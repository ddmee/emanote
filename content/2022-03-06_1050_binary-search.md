# Binary Search

Improvement over [2022-03-06_1045_linear-search] provided list is sorted.

Binary search, where the input is reduced in half each iteration (thus binary, because split in two halves, throwing away one half per iteration). Halving the input everytime means the growth increases by 1 iteration as input doubles. I.e. 1,2,4,8,16,32,64,128 elements etc. (Note here it's the growth of the length of the list we care about) So the growth rate the inverse of `n**2` which is `O(log2(length(n)))` log base 2 of length of n:

```python
def bSearch(L, e, low, high):
    if high - low < 2:
        return L[low] == e or L[high] == e
    mid = low + int((high - low)/2)
    if L[mid] == e:
        return True
    if L[mid] > e:
        return bSearch(L, e, low, mid - 1)
    else:
        return bSearch(L, e, mid + 1, high)


def search(L, e):
    return bSearch(L, e, 0, len(L) - 1)
```

Note binary search requires an order list as input. We are ignoring here the cost of sorting the list from the cost of the algorithm.

And of course the other thing is this complexity calculation assumes that accessing the list elements like `L[mid]` is constant which depends on the programming language providing that feature. Constant lookup times are straight-forward if the elements of a list are fixed sizes in memory. However, for lists where elements can be of variable size, the implementation of a such a list, the [[2022-03-08_1643_linked_lists]] is not constant look-up time.

## Dynamic array and indirection

Instead, you have a list of pointers. Each pointer is of fixed size. The pointer points to the element in some other memory location. Thus to lookup I(th) element, it's constant time because you can exactly calculate the offset for the location of the pointer and the pointer then points to the element.

How do you know the size of the value pointed at? Do you have to keep a second value, the size of the memory block for the element? You could compare the pointer with the next pointer, assuming the list is kept in continous memory locations. But what about if you need to change the list so it has a new value at an existing location that has a different memory size?

This is called a [[2022-03-08_1726_dynamic-array]]

## Sorting the list

Binary search assumes the list is sorted. How costly is it to sort? And is sorting worth it?

With linear search, it costs `O(length(list))` and works on unsorted lists. Binary search is `O(log(length(list)))`. So, if we have an unsorted list, it must be cost `O(log(length(list)))` or less to sort, as anything more expensive than that [[2022-03-06_1040_graphs_of_growth]] is a linear cost.

Do you think it's possible to sort a list in time less than linear cost? Well, doesn't seem so. Because to order the list, we'd at least need to visit every item in the list once, which is then order linear length of the list. No, there is provably no algorithm that can sort a list in sub-linear time.

See [[2022-03-08_1816_sorting-a-list]]

## Amortised complexity

Sorting a list is relatively expensive but if we are going to search the list a lot, then the cost of the sort can be said to be 'amortised' across all the opportunities to before a binary search instead of a linear search.

We want to ask, *if we plan on performing K searches, is the order of sorting the list plus K times log(length(list)) less than K times length(list)*? The equation:

```
O(sort(L)) + K * log(len(L)) < K * len(L)
```
I.e., that's:

```
cost to sort the list + (K * binary search of the list) < K linear searches of an unordered list
```

In reality, to know this, we need to know how many times we are going to query the list. And, we'd need to query the list a lot to produce a time saving. But in practice, there might be a lot of examples of this amortisable cost.

#complexity
#algorithms
#search
