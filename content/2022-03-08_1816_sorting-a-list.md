# Sorting a list

To sort a list, you must visit each element of the list at least once. Thus we know we cannot produce a sorting algorithm that is faster than `O(length(list))` (which is linear growth).

In fact, the fastest algorithm for sorting a list is currently Log-linear. (See [[2022-03-06_1040_graphs_of_growth]])

## Bad algorithms, O(n squared)

A *bad* algorithm for sorting a list is [[bubble sort|2022-03-08_1851_bubble-sort]] because it's `O(n**2)` where n is the number of elements in the list.

[[Selection sort|2022-03-08_1843_selection-sort]] is a better algorithm but it's still `O(n**2)`. It's better because it's `O(0.5*n**2)` as the unordered part of the list we iterate over shrinks 1 by each iteration. However, because we ignore multiplicative constants in BigO analysis, it's considered equivalent to O n squared.

Can we do better?

## Divide and conquer algorithms

General pattern:

1. Choose a threshold size, that's the smallest size of the problem.

2. How many instances are we going to divide it into?

3. Algorithm to combine the subsolutions into the larger solution

See [divide and conquer on wiki](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm)

[[Merge sort|2022-03-08_2109_merge-sort]] is an example. It's `O(nlog(n))` where n is the length of the list (a.k.a log linear growth).

#complexity
#algorithms
#list
#sort
